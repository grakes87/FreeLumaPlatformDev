---
phase: 12-content-production-platform
plan: 03
type: execute
wave: 2
depends_on: ["12-01", "12-02"]
files_modified:
  - src/lib/content-pipeline/text-generation.ts
  - src/lib/content-pipeline/tts-elevenlabs.ts
  - src/lib/content-pipeline/tts-murf.ts
  - src/lib/content-pipeline/srt-generator.ts
autonomous: true

must_haves:
  truths:
    - "Claude API generates positivity quotes, devotional reflections, camera scripts, meditation scripts, and background video prompts"
    - "ElevenLabs TTS returns audio buffer + character alignment for SRT generation"
    - "Murf TTS returns audio URL + word durations for SRT generation"
    - "SRT generator converts timing data to valid .srt format string"
  artifacts:
    - path: "src/lib/content-pipeline/text-generation.ts"
      provides: "All Claude AI text generation functions"
      exports: ["generatePositivityQuote", "generateDevotionalReflection", "generateCameraScript", "generateMeditationScript", "generateBackgroundPrompt"]
    - path: "src/lib/content-pipeline/tts-elevenlabs.ts"
      provides: "ElevenLabs TTS with character-level timestamps"
      exports: ["generateTtsElevenLabs"]
    - path: "src/lib/content-pipeline/tts-murf.ts"
      provides: "Murf REST API TTS with word durations"
      exports: ["generateTtsMurf"]
    - path: "src/lib/content-pipeline/srt-generator.ts"
      provides: "SRT file generation from timing data"
      exports: ["generateSrt", "characterAlignmentToWords", "murfDurationsToWords"]
  key_links:
    - from: "src/lib/content-pipeline/tts-elevenlabs.ts"
      to: "src/lib/content-pipeline/srt-generator.ts"
      via: "character alignment -> word timing -> SRT"
      pattern: "characterAlignmentToWords"
    - from: "src/lib/content-pipeline/tts-murf.ts"
      to: "src/lib/content-pipeline/srt-generator.ts"
      via: "word durations -> word timing -> SRT"
      pattern: "murfDurationsToWords"
---

<objective>
Build the core content pipeline library modules: AI text generation (Claude), TTS audio (ElevenLabs + Murf), and SRT subtitle generation.

Purpose: These modules are the building blocks the pipeline runner uses to generate daily content. Each is a focused, testable function that the pipeline runner orchestrates.
Output: 4 library modules covering all text, audio, and subtitle generation.
</objective>

<execution_context>
@/Users/admin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/admin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/12-content-production-platform/12-CONTEXT.md
@.planning/phases/12-content-production-platform/12-RESEARCH.md
@src/app/api/admin/verse-generation/route.ts
@src/lib/db/models/PlatformSetting.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AI text generation module and SRT generator</name>
  <files>
    src/lib/content-pipeline/text-generation.ts
    src/lib/content-pipeline/srt-generator.ts
  </files>
  <action>
**text-generation.ts** -- Claude API text generation for all content types:

Follow the existing Anthropic API pattern from verse-generation/route.ts. Use `@anthropic-ai/sdk` (already installed). Use `claude-sonnet-4-20250514` model (established in codebase).

Create 5 exported async functions, each taking relevant context and returning generated text:

1. `generatePositivityQuote()`: Returns an original motivational/inspirational quote (1-3 sentences). Prompt should instruct Claude to create an original quote (not a famous quote), positive and uplifting, suitable for a daily devotional app.

2. `generateDevotionalReflection(verseReference: string, verseText: string)`: Returns 4-6 sentence devotional reflection on the verse. Prompt should instruct for warm, accessible tone suitable for daily meditation.

3. `generateCameraScript(mode: 'bible' | 'positivity', content: { verseReference?: string; verseText?: string; quote?: string })`: Returns ~45-second spoken camera script. For bible mode: deep-dive context on the verse. For positivity mode: expand on the quote. Prompt should specify: conversational and warm tone, like a friend sharing an insight, approximately 45 seconds when read aloud (~110-120 words).

4. `generateMeditationScript(mode: 'bible' | 'positivity', content: { verseReference?: string; verseText?: string; quote?: string })`: Returns ~1-minute guided meditation script. Structure: opening breath -> visualization -> affirmation -> closing. Prompt should specify the template structure and target ~1 minute read time (~150-160 words).

5. `generateBackgroundPrompt(mode: 'bible' | 'positivity', content: { verseReference?: string; verseText?: string; quote?: string })`: Returns detailed cinematic video prompt for background. Prompt should specify: detailed camera angles, mood, color palette, lighting. Must NOT include people -- scenery only. Suitable for Sora/Veo video generation.

Each function:
- Gets ANTHROPIC_API_KEY from process.env (not PlatformSetting -- API key is env-level)
- Creates Anthropic client
- Makes messages.create call with focused prompt
- Extracts text from response.content[0]
- max_tokens: 1024 for all (these are short outputs)
- Returns the generated text string
- Throws on API error (caller handles retries)

**srt-generator.ts** -- SRT subtitle file generation:

Define `WordTiming` interface: `{ word: string; startTime: number; endTime: number }` (times in seconds).

Export `formatSrtTime(seconds: number): string` -- converts seconds to "HH:MM:SS,mmm" SRT format.

Export `generateSrt(words: WordTiming[], maxWordsPerLine: number = 8): string` -- groups words into subtitle lines of maxWordsPerLine words each, formats as valid SRT with sequential index numbers.

Export `characterAlignmentToWords(alignment: { characters: string[]; character_start_times_seconds: number[]; character_end_times_seconds: number[] }): WordTiming[]` -- converts ElevenLabs character-level alignment to word-level timing by grouping characters between whitespace boundaries.

Export `murfDurationsToWords(wordDurations: Array<{ word: string; start: number; end: number }>): WordTiming[]` -- normalizes Murf word duration format to WordTiming[]. If Murf returns only `duration` (not start/end), compute cumulative start times. Handle both formats defensively.
  </action>
  <verify>
Run `npx tsc --noEmit src/lib/content-pipeline/text-generation.ts src/lib/content-pipeline/srt-generator.ts` to verify TypeScript compilation. Test SRT generation manually: `npx tsx -e "import { generateSrt } from './src/lib/content-pipeline/srt-generator'; console.log(generateSrt([{word:'Hello',startTime:0,endTime:0.5},{word:'world',startTime:0.6,endTime:1.0}]))"` should output valid SRT format.
  </verify>
  <done>5 Claude text generation functions created (quote, devotional, camera script, meditation, background prompt). SRT generator converts both ElevenLabs character alignment and Murf word durations to valid .srt format.</done>
</task>

<task type="auto">
  <name>Task 2: Create ElevenLabs and Murf TTS modules</name>
  <files>
    src/lib/content-pipeline/tts-elevenlabs.ts
    src/lib/content-pipeline/tts-murf.ts
  </files>
  <action>
**tts-elevenlabs.ts** -- English TTS via ElevenLabs SDK:

```typescript
export async function generateTtsElevenLabs(
  text: string,
  voiceId: string,
  apiKey: string
): Promise<{ audioBuffer: Buffer; alignment: { characters: string[]; character_start_times_seconds: number[]; character_end_times_seconds: number[] } }>
```

Implementation:
- Import `ElevenLabsClient` from `@elevenlabs/elevenlabs-js`
- Create client with apiKey
- Try SDK method first: `client.textToSpeech.convertWithTimestamps(voiceId, { text, model_id: 'eleven_multilingual_v2', output_format: 'mp3_44100_128' })`
- If SDK method name differs (TypeScript will catch this), fall back to direct REST fetch to `https://api.elevenlabs.io/v1/text-to-speech/${voiceId}/with-timestamps` with POST body and xi-api-key header
- Parse response: audio_base64 -> Buffer.from(base64, 'base64'), alignment object
- Return { audioBuffer, alignment }
- On 429 rate limit: throw a specific RateLimitError with retry-after header value if available

**tts-murf.ts** -- Spanish/non-English TTS via Murf REST API:

```typescript
export async function generateTtsMurf(
  text: string,
  voiceId: string,
  apiKey: string
): Promise<{ audioBuffer: Buffer; wordDurations: Array<{ word: string; start: number; end: number }> }>
```

Implementation:
- Direct REST call to `https://api.murf.ai/v1/speech/generate`
- Headers: `{ 'api-key': apiKey, 'Content-Type': 'application/json' }`
- Body: `{ text, voiceId, format: 'MP3' }`
- Response includes: `{ audioFile: string (URL), wordDurations: [...] }`
- Fetch the audio from audioFile URL to get Buffer
- Normalize wordDurations: if items have `start`/`end`, use directly. If items have only `duration`, compute cumulative start/end.
- Return { audioBuffer, wordDurations }
- On non-200 response, throw with status and error message

Both modules:
- Get voice IDs and API keys from PlatformSetting (passed as params by caller)
- Are pure functions (no DB access themselves -- caller passes config)
- Include type exports for their response shapes
  </action>
  <verify>
Run `npx tsc --noEmit src/lib/content-pipeline/tts-elevenlabs.ts src/lib/content-pipeline/tts-murf.ts` to verify TypeScript compilation without errors.
  </verify>
  <done>ElevenLabs TTS module generates English audio with character-level timestamps for SRT. Murf TTS module generates Spanish audio with word-level durations via REST API. Both return audio Buffer + timing data.</done>
</task>

</tasks>

<verification>
- All 4 files compile without TypeScript errors
- SRT generation produces valid .srt format with correct timing
- Text generation functions have correct signatures and prompt structures
- TTS modules handle both success and error cases
</verification>

<success_criteria>
- 5 AI text generation functions cover all content types (quote, devotional, camera script, meditation, background prompt)
- ElevenLabs TTS returns audio + character alignment
- Murf TTS returns audio + word durations
- SRT generator converts both alignment formats to valid subtitle files
- All modules use existing patterns (Anthropic SDK, PlatformSetting for config)
</success_criteria>

<output>
After completion, create `.planning/phases/12-content-production-platform/12-03-SUMMARY.md`
</output>
